{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50b8a25",
   "metadata": {},
   "source": [
    "# CPU Model Runner\n",
    "\n",
    "Assuming you have models saved at `model_cpu/bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2d04b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e0ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\Documents\\UT\\Fa25\\ECE 382V Parallel Algorithms\\Project\\ParallelConvolution\\.venv\\Scripts\\python.exe\n",
      "3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbab8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_ocl\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from model_ocl.model import SimpleCNN_OCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613ec3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3353, -1.9969,  0.4002, -0.8788,  0.5879,  0.2263,  2.3304, -0.4758,\n",
      "          0.6576, -0.6822],\n",
      "        [ 0.8969,  0.5318, -1.6782,  0.5136, -1.2366,  0.0815, -0.2332, -0.5884,\n",
      "          0.2860,  1.3909],\n",
      "        [-0.5703,  0.2103,  0.6810,  1.4556, -1.4839,  0.8884,  0.1283,  0.3658,\n",
      "         -1.5041,  1.4942],\n",
      "        [ 0.5756, -0.1858, -1.8230, -0.3469, -0.2627, -1.7218,  0.9533, -0.1594,\n",
      "          0.8503, -0.9109],\n",
      "        [ 0.9801, -0.3053,  0.6675,  0.1577,  0.9398, -2.0231, -0.2284,  0.0171,\n",
      "          0.9087, -0.6431],\n",
      "        [-0.2854, -1.4053,  1.3438, -1.7106,  1.0479, -0.5900, -0.6511,  0.7411,\n",
      "         -0.7643, -1.1028],\n",
      "        [-0.3865,  1.3572,  0.8877, -0.5633, -0.0361,  0.6555, -0.8055, -0.0862,\n",
      "          1.9238, -0.6474],\n",
      "        [-0.6947,  0.1900, -1.3436,  1.0144,  0.4820, -1.3304,  1.8510, -0.5527,\n",
      "          0.9221,  1.0242],\n",
      "        [ 0.1837,  0.9953,  1.8329, -0.6770, -1.2779, -0.2813,  1.3593, -0.3489,\n",
      "         -0.7107, -0.1647],\n",
      "        [ 0.6120,  2.1700, -0.7601, -0.8759, -0.0914,  0.9442,  0.2508, -0.0840,\n",
      "         -0.4722, -1.4013]], device='ocl:1')\n"
     ]
    }
   ],
   "source": [
    "# test if opencl backend is working\n",
    "d = \"ocl:1\"\n",
    "a = torch.randn(10, 10, device=d)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3891d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "SAVE_DIR = \"model_ocl/bin\"\n",
    "\n",
    "FP32_MODEL_NAME = \"CNN-MNIST-OCL1-fp32.pt\"\n",
    "FP32_SAVE_PATH = os.path.join(SAVE_DIR, FP32_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecd8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters and Setup ---\n",
    "DEVICE = torch.device(d)   # <-- USE OPENCL GPU\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 10\n",
    "TRAIN_VAL_SPLIT_RATIO = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ca3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Data and Define DataLoaders ---\n",
    "def get_dataloaders():\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    full_train_dataset = datasets.MNIST(\n",
    "        root=\"./data\",\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root=\"./data\",\n",
    "        train=False,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    train_size = int(len(full_train_dataset) * TRAIN_VAL_SPLIT_RATIO)\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "    train_subset, val_subset = random_split(\n",
    "        full_train_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52bf2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: ocl:1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "model = SimpleCNN_OCL().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd76af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Comparison ---\n",
    "# Helper to print model size\n",
    "def print_model_size(model, label):\n",
    "    # Save a temporary file\n",
    "    torch.save(model.state_dict(), \"temp.pt\")\n",
    "    size_mb = os.path.getsize(\"temp.pt\") / (1024 * 1024)\n",
    "    print(f\"Size of {label} model: {size_mb:.2f} MB\")\n",
    "    os.remove(\"temp.pt\")\n",
    "\n",
    "# Helper to evaluate accuracy\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Helper to measure inference speed (Throughput)\n",
    "def measure_inference_speed(model, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Use one batch for a warm-up/test run\n",
    "    dummy_input = next(iter(data_loader))[0].to(device)\n",
    "    \n",
    "    # Warm-up runs\n",
    "    print(\"  Running warm-up...\")\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "        \n",
    "    # Measure\n",
    "    print(\"  Measuring inference throughput...\")\n",
    "    start_time = time.time()\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            _ = model(images.to(device))\n",
    "            # Add the batch size (number of images in this batch)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    samples_per_second = total_samples / total_time\n",
    "    \n",
    "    return samples_per_second\n",
    "\n",
    "# Helper to get model size in bytes\n",
    "def get_model_size(model):\n",
    "    \"\"\"Saves model state_dict temporarily to get file size.\"\"\"\n",
    "    # Save a temporary file\n",
    "    torch.save(model.state_dict(), \"temp_size_calc.pt\")\n",
    "    size_bytes = os.path.getsize(\"temp_size_calc.pt\")\n",
    "    os.remove(\"temp_size_calc.pt\")\n",
    "    return size_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776791db",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c4df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FP32 model from 'model_ocl/bin\\CNN-MNIST-OCL1-fp32.pt'...\n",
      "FP32 Model Accuracy: 99.26%\n",
      "Measuring FP32 model...\n",
      "  Running warm-up...\n",
      "  Measuring inference throughput...\n",
      "-> FP32 Throughput: 3064.53 samples/sec\n"
     ]
    }
   ],
   "source": [
    "# --- Load the FP32 model ---\n",
    "print(f\"Loading FP32 model from '{FP32_SAVE_PATH}'...\")\n",
    "fp32_model = SimpleCNN_OCL().to(DEVICE)\n",
    "fp32_model.load_state_dict(\n",
    "    torch.load(FP32_SAVE_PATH)\n",
    ")\n",
    "fp32_model.eval()\n",
    "\n",
    "fp32_accuracy = evaluate_model(fp32_model, test_loader, DEVICE)\n",
    "print(f\"FP32 Model Accuracy: {fp32_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Measuring FP32 model...\")\n",
    "fp32_throughput = measure_inference_speed(fp32_model, val_loader, DEVICE)\n",
    "print(f\"-> FP32 Throughput: {fp32_throughput:.2f} samples/sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
