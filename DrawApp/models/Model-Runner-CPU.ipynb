{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50b8a25",
   "metadata": {},
   "source": [
    "# CPU Model Runner\n",
    "\n",
    "Assuming you have models saved at `model_cpu/bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2d04b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e0ff00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\python.exe\n",
      "3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbab8ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdbd38e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_THREADS = 2\n",
    "torch.set_num_threads(NUM_THREADS)         # Affects intra-op parallelism (e.g., GEMM)\n",
    "torch.set_num_interop_threads(NUM_THREADS) # Affects inter-op parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba7b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_cpu.model import SimpleCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3891d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and file path\n",
    "SAVE_DIR = \"model_cpu/bin\"\n",
    "\n",
    "FP32_MODEL_NAME = \"CNN-MNIST-CPU-fp32.pt\"\n",
    "FP32_SAVE_PATH = os.path.join(SAVE_DIR, FP32_MODEL_NAME)\n",
    "\n",
    "INT8_MODEL_NAME = \"CNN-MNIST-CPU-int8.pt\"\n",
    "INT8_SAVE_PATH = os.path.join(SAVE_DIR, INT8_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cecd8714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Hyperparameters and Setup ---\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "TRAIN_VAL_SPLIT_RATIO = 0.9 # 90% for training, 10% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40ca3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load Data and Define DataLoaders ---\n",
    "def get_dataloaders():\n",
    "    # Standard transforms for MNIST\n",
    "    # ToTensor() scales images from [0, 255] to [0.0, 1.0]\n",
    "    # Normalize() standardizes the data. (0.1307,) and (0.3081,) are\n",
    "    # the pre-calculated mean and std dev of the MNIST dataset.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Download the full training dataset (60,000 images)\n",
    "    full_train_dataset = datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=True, \n",
    "        transform=transform, \n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    # Download the test dataset (10,000 images)\n",
    "    test_dataset = datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=False, \n",
    "        transform=transform, \n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "    # Split the full training dataset into training and validation sets\n",
    "    train_size = int(len(full_train_dataset) * TRAIN_VAL_SPLIT_RATIO)\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    \n",
    "    train_subset, val_subset = random_split(\n",
    "        full_train_dataset, \n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42) # for reproducible splits\n",
    "    )\n",
    "    \n",
    "    print(f\"Total training images: {len(train_subset)}\")\n",
    "    print(f\"Total validation images: {len(val_subset)}\")\n",
    "    print(f\"Total test images: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create DataLoaders for each set\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_subset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_subset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52bf2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Total training images: 54000\n",
      "Total validation images: 6000\n",
      "Total test images: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {DEVICE}\")\n",
    "    \n",
    "# Get DataLoaders\n",
    "train_loader, val_loader, test_loader = get_dataloaders()\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = SimpleCNN().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss() # Combines LogSoftmax and NLLLoss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# History lists to store metrics\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "val_loss_history = []\n",
    "val_acc_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd76af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions for Comparison ---\n",
    "# Helper to print model size\n",
    "def print_model_size(model, label):\n",
    "    # Save a temporary file\n",
    "    torch.save(model.state_dict(), \"temp.pt\")\n",
    "    size_mb = os.path.getsize(\"temp.pt\") / (1024 * 1024)\n",
    "    print(f\"Size of {label} model: {size_mb:.2f} MB\")\n",
    "    os.remove(\"temp.pt\")\n",
    "\n",
    "# Helper to evaluate accuracy\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    return 100 * correct / total\n",
    "\n",
    "# Helper to measure inference speed (Throughput)\n",
    "def measure_inference_speed(model, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Use one batch for a warm-up/test run\n",
    "    dummy_input = next(iter(data_loader))[0].to(device)\n",
    "    \n",
    "    # Warm-up runs\n",
    "    print(\"  Running warm-up...\")\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy_input)\n",
    "        \n",
    "    # Measure\n",
    "    print(\"  Measuring inference throughput...\")\n",
    "    start_time = time.time()\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in data_loader:\n",
    "            _ = model(images.to(device))\n",
    "            # Add the batch size (number of images in this batch)\n",
    "            total_samples += images.size(0)\n",
    "            \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    samples_per_second = total_samples / total_time\n",
    "    \n",
    "    return samples_per_second\n",
    "\n",
    "# Helper to get model size in bytes\n",
    "def get_model_size(model):\n",
    "    \"\"\"Saves model state_dict temporarily to get file size.\"\"\"\n",
    "    # Save a temporary file\n",
    "    torch.save(model.state_dict(), \"temp_size_calc.pt\")\n",
    "    size_bytes = os.path.getsize(\"temp_size_calc.pt\")\n",
    "    os.remove(\"temp_size_calc.pt\")\n",
    "    return size_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776791db",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c4df76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing FP32 and INT8 Models ---\n",
      "Loading FP32 model from 'model_cpu/bin\\CNN-MNIST-CPU-fp32.pt'...\n",
      "Loading INT8 model from 'model_cpu/bin\\CNN-MNIST-CPU-int8.pt'...\n",
      "Models loaded.\n",
      "\n",
      "--- Model Size Comparison ---\n",
      "Size of FP32 model: 1.74 MB\n",
      "Size of INT8 model: 0.45 MB\n",
      "\n",
      "--- Accuracy Comparison (on Test Set) ---\n",
      "FP32 Model Accuracy: 99.17%\n",
      "INT8 Model Accuracy: 99.16%\n",
      "\n",
      "--- Inference Speed Comparison (on Validation Set) ---\n",
      "Measuring FP32 model...\n",
      "  Running warm-up...\n",
      "  Measuring inference throughput...\n",
      "-> FP32 Throughput: 1429.40 samples/sec\n",
      "\n",
      "Measuring INT8 model...\n",
      "  Running warm-up...\n",
      "  Measuring inference throughput...\n",
      "-> INT8 Throughput: 1938.85 samples/sec\n",
      "\n",
      "--- Summary ---\n",
      "Size Reduction: ~74.3%\n",
      "Speedup: 1.36x\n",
      "Accuracy Drop: 0.01%\n"
     ]
    }
   ],
   "source": [
    "# --- Run the Comparison ---\n",
    "print(\"\\n--- Comparing FP32 and INT8 Models ---\")\n",
    "\n",
    "# (Ensure test_loader and val_loader are loaded)\n",
    "if 'test_loader' not in locals():\n",
    "    print(\"Loading data for evaluation...\")\n",
    "    _, val_loader, test_loader = get_dataloaders()\n",
    "    \n",
    "# --- 1. Load the original FP32 model ---\n",
    "print(f\"Loading FP32 model from '{FP32_SAVE_PATH}'...\")\n",
    "fp32_model = SimpleCNN().to(DEVICE)\n",
    "fp32_model.load_state_dict(\n",
    "    torch.load(FP32_SAVE_PATH), \n",
    "    strict=False # For quant/dequant stubs\n",
    ")\n",
    "fp32_model.eval()\n",
    "\n",
    "# --- 2. Load the saved INT8 model ---\n",
    "print(f\"Loading INT8 model from '{INT8_SAVE_PATH}'...\")\n",
    "# To load a quantized model, you MUST recreate the same\n",
    "# fused and quantized structure *before* loading the state_dict.\n",
    "int8_model_to_load = SimpleCNN().to(DEVICE)\n",
    "int8_model_to_load.eval()\n",
    "int8_model_to_load.fuse_model() # Fuse first\n",
    "int8_model_to_load.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(int8_model_to_load, inplace=True)\n",
    "torch.quantization.convert(int8_model_to_load, inplace=True) \n",
    "\n",
    "# Now load the saved INT8 state dict\n",
    "int8_model_to_load.load_state_dict(\n",
    "    torch.load(INT8_SAVE_PATH)\n",
    ")\n",
    "int8_model = int8_model_to_load\n",
    "print(\"Models loaded.\")\n",
    "\n",
    "# --- 3. Compare Model Size ---\n",
    "print(\"\\n--- Model Size Comparison ---\")\n",
    "print_model_size(fp32_model, \"FP32\")\n",
    "print_model_size(int8_model, \"INT8\")\n",
    "\n",
    "# --- 4. Compare Accuracy ---\n",
    "print(\"\\n--- Accuracy Comparison (on Test Set) ---\")\n",
    "fp32_accuracy = evaluate_model(fp32_model, test_loader, DEVICE)\n",
    "print(f\"FP32 Model Accuracy: {fp32_accuracy:.2f}%\")\n",
    "\n",
    "int8_accuracy = evaluate_model(int8_model, test_loader, DEVICE)\n",
    "print(f\"INT8 Model Accuracy: {int8_accuracy:.2f}%\")\n",
    "\n",
    "# --- 5. Compare Inference Speed ---\n",
    "print(\"\\n--- Inference Speed Comparison (on Validation Set) ---\")\n",
    "print(\"Measuring FP32 model...\")\n",
    "fp32_throughput = measure_inference_speed(fp32_model, val_loader, DEVICE)\n",
    "print(f\"-> FP32 Throughput: {fp32_throughput:.2f} samples/sec\")\n",
    "\n",
    "print(\"\\nMeasuring INT8 model...\")\n",
    "int8_throughput = measure_inference_speed(int8_model, val_loader, DEVICE)\n",
    "print(f\"-> INT8 Throughput: {int8_throughput:.2f} samples/sec\")\n",
    "\n",
    "# --- 6. Summary ---\n",
    "print(\"\\n--- Summary ---\")\n",
    "fp32_size_bytes = get_model_size(fp32_model)\n",
    "int8_size_bytes = get_model_size(int8_model)\n",
    "\n",
    "print(f\"Size Reduction: ~{((1 - (int8_size_bytes / fp32_size_bytes)) * 100):.1f}%\")\n",
    "# Speedup calculation: Faster / Slower\n",
    "print(f\"Speedup: {int8_throughput / fp32_throughput:.2f}x\")\n",
    "print(f\"Accuracy Drop: {fp32_accuracy - int8_accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
